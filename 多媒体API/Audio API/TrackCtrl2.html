<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Web Audio API examples: MediaElementAudioSource()</title>
    <style>
      body {
        padding: 100px;
      }
      input[type="range"] {
        transform: rotate(-90deg);
      }
    </style>
  </head>

  <body>
    <h1>Web Audio API examples: createScriptProcessor()</h1>

    <br />
    <br />
    <button type="button" id="LBTN">静音左声道</button>
    <input type="range" id="L" min="0" max="100" />
    <audio mute controls>
      <source src="./media/hlbb.mp3" type="audio/mp3" />
      <source src="./media/viper.ogg" type="audio/ogg" />
      <p>This demo needs a browser supporting the &lt;audio&gt; element.</p>
    </audio>
    <button type="button" id="PBTN">音频播放</button>
    <video mute controls width="500">
      <source src="./media//202107.mp4" type="video/mp4" />
      <p>This demo needs a browser supporting the &lt;video&gt; element.</p>
    </video>
    <input type="range" id="R" min="0" max="100" />

    <button type="button" id="RBTN">静音右声道</button>

    <video id="video2" mute controls width="500">
      <source src="./media//202107.mp4" type="video/mp4" />
      <p>This demo needs a browser supporting the &lt;video&gt; element.</p>
    </video>
    <canvas id="canvas"></canvas>
    <script>
      {
        const ac = new (window.AudioContext || window.webkitAudioContext)();

        // const audio = new Audio('./media/hlbb.mp3');

        // const audio = document.querySelector("audio");

        const audio = document.querySelector("video");

        const L = document.querySelector("#L");
        const R = document.querySelector("#R");

        let source;
        let audioData = [];

        // audio.addEventListener(
        //   "canplaythrough", // loadedmetadata
        //   function () {
        //     source = ac.createMediaElementSource(audio);
        //     source.connect(processor);
        //     source.connect(ac.destination);
        //     processor.connect(ac.destination);

        //     audio.play();
        //   },
        //   false
        // );

        audio.crossOrigin = "anonymous"; // cross-origin - if file is stored on remote server

        const channelCount = 2; // or read from: 'audioSource.channelCount'

        const audioSource = ac.createMediaElementSource(audio);

        const processor = ac.createScriptProcessor(2048, 1, 1);

        audioSource.connect(processor);
        // audioSource.connect(ac.destination);
        processor.connect(ac.destination);

        const volumeNodeL = new GainNode(ac);
        const volumeNodeR = new GainNode(ac);

        volumeNodeL.gain.value = 2;
        volumeNodeR.gain.value = 2;

        // 通道分配器
        const splitterNode = new ChannelSplitterNode(ac, {
          numberOfOutputs: channelCount,
        });

        // 通道合并器
        const mergerNode = new ChannelMergerNode(ac, {
          numberOfInputs: channelCount,
        });

        audioSource.connect(splitterNode);

        splitterNode.connect(volumeNodeL, 0); // connect OUTPUT channel 0
        splitterNode.connect(volumeNodeR, 1); // connect OUTPUT channel 1

        volumeNodeL.connect(mergerNode, 0, 0); // connect INPUT channel 0
        volumeNodeR.connect(mergerNode, 0, 1); // connect INPUT channel 1

        mergerNode.connect(ac.destination);

        // 循环PCM数据并计算平均值
        // 给定2048样本缓冲区的体积
        processor.onaudioprocess = function (evt) {
          {
            let gcd = evt.inputBuffer.getChannelData(0),
              len = gcd.length,
              total = (i = 0),
              rms;
            while (i < len) {
              total += Math.abs(gcd[i++]);
            }
            rms = Math.sqrt(total / len);

            // console.log(input, rms, rms * 100);
            L.value = rms * 100;
            R.value = rms * 100;

            // console.log(evt.inputBuffer);

            console.log("音频通道数量：", evt.inputBuffer.numberOfChannels);
            console.log("音频通道数量：", evt.inputBuffer.getChannelData(1));
          }

          // {
          //   let left = evt.inputBuffer.getChannelData(0);
          //   let right = evt.inputBuffer.getChannelData(1);

          //   console.lg(left, right, evt.inputBuffer.getChannelData());
          //   // 对左右声道进行处理
          //   // 例如，计算音量大小
          //   for (let i = 0; i < left.length; i++) {
          //     audioData[0][i] = left[i];
          //     audioData[1][i] = right[i];
          //   }
          //   console.log(audioData);
          // }
        };

        {
          //   const audio = new Audio("./media/202107.mp4");

          let isPlaying;
          function playPause() {
            // check if context is in suspended state (autoplay policy)
            if (ac.state === "suspended") {
              ac.resume();
            }

            isPlaying = !isPlaying;
            if (isPlaying) {
              audio.play();
            } else {
              audio.pause();
            }
          }

          LBTN.onclick = function () {
            volumeNodeL.gain.value = Number(!volumeNodeL.gain.value);
            console.log(volumeNodeL.gain.value);
          };

          PBTN.onclick = function () {
            playPause();
          };

          RBTN.onclick = function () {
            volumeNodeR.gain.value = Number(!volumeNodeR.gain.value);
            console.log(volumeNodeR.gain.value);
          };
        }

        {
          let audioContext = new (window.AudioContext ||
            window.webkitAudioContext)();
          let analyser = audioContext.createAnalyser();

          let videoElement = document.getElementById("video2");
          let audioOutput = videoElement.audioOutput;
          audioOutput.connect(analyser);
          let bufferSize = analyser.frequencyBinCount;
          let arrayLength = bufferSize;
          let byteArray = new Uint8Array(arrayLength);
          let canvas = document.getElementById("canvas"); // 你的Canvas元素
          let canvasCtx = canvas.getContext("2d");
          let drawRequest = new Image();
          drawRequest.src = canvas.toDataURL();
          drawRequest.onload = function () {
            let image = drawRequest;
            canvasCtx.drawImage(image, 0, 0);
          };
          let update = function () {
            analyser.getByteTimeDomainData(byteArray);
            let value = byteArray[0]; // 取第一个值，即音量值
            let progress = value / 100.0; // 将音量值映射为进度条的比例值（这里为100）
            canvasCtx.fillStyle =
              "rgb(" + parseInt(progress * 255) + ",100,50)"; // 根据进度条的比例值改变颜色
            canvasCtx.fillRect(0, 0, canvas.width, canvas.height); // 填充整个Canvas
            requestAnimationFrame(update); // 持续更新进度条
          };
          update();
        }
      }

      {
        // 创建一个AudioContext
        const ac = new (window.AudioContext || window.webkitAudioContext)();

        // 加载音频资源
        fetch("./media/AAC-5.1.mp4")
          .then((response) => {
            return response.arrayBuffer();
          })
          .then((arrayBuffer) => {
            return ac.decodeAudioData(arrayBuffer);
          })
          .then((audioBuffer) => {
            // 获取音频源并创建两个独立的音频源
            let sourceLeft = ac.createBufferSource();
            // let sourceRight = ac.createBufferSource();

            // 分别将音频数据赋值给左右两个音频源
            sourceLeft.buffer = audioBuffer;
            // sourceRight.buffer = audioBuffer;

            // 创建两个分轨并分别连接至音频源
            let trackLeft = ac.createGain();
            // let trackRight = ac.createGain();

            sourceLeft.connect(trackLeft);
            // sourceRight.connect(trackRight);

            trackLeft.connect(ac.destination);
            // trackRight.connect(ac.destination);

            // 开始播放
            // sourceLeft.start();
            // sourceRight.start();
          });
      }
    </script>
  </body>
</html>
