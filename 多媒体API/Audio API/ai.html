<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <style>
      body{ padding: 100px; }
      input[type="range"] {
        transform: rotate(-90deg);
      }
    </style>
  </head>
  <body>
    <video id="myVideo" width="320" height="240" controls>
      <source src="./media/202107.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>

    <p id="leftChannelLevel"></p>
    <p id="rightChannelLevel"></p>

    <button onclick="playLeft()">播放左声道</button>
    <button onclick="pauseLeft()">暂停左声道</button>
    <button onclick="playRight()">播放右声道</button>
    <button onclick="pauseRight()">暂停右声道</button>

    <canvas id="canvas" width="400" height="200"></canvas>

    <script>
      var audioContext = new  (window.AudioContext || window.webkitAudioContext)();
      var video = document.getElementById("myVideo");
      var sourceNode = audioContext.createMediaElementSource(video);
      var analyserNode = audioContext.createAnalyser();
      var canvas = document.getElementById("canvas");
      var ctx = canvas.getContext("2d");
      var leftChannelLevel = 0;
      var rightChannelLevel = 0;
      var isPlayingLeft = false;
      var isPlayingRight = false;

      // 将视频的音频流连接到音频上下文和分析器节点
      sourceNode.connect(analyserNode);
      analyserNode.connect(audioContext.destination);

      // 获取音频数据的频率和时间域数据
      analyserNode.fftSize = 2048;
      var bufferLength = analyserNode.frequencyBinCount;
      var dataArrayTime = new Uint8Array(bufferLength);
      var dataArrayFreq = new Uint8Array(bufferLength);
      analyserNode.getByteFrequencyData(dataArrayFreq);
      analyserNode.getByteTimeDomainData(dataArrayTime);

      // 绘制音频响度的可视化图形
      function draw() {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.fillStyle = "rgb(0, 0, 0)";
        ctx.fillRect(0, 0, canvas.width, canvas.height);
        ctx.fillStyle = "rgb(255, 255, 255)";
        ctx.fillText("Left Channel Level: " + leftChannelLevel, 10, 20);
        ctx.fillText("Right Channel Level: " + rightChannelLevel, 10, 40);
        ctx.fillStyle = "rgb(0, 255, 0)";
        ctx.fillRect(
          (canvas.width * leftChannelLevel) / bufferLength,
          0,
          canvas.width * (rightChannelLevel / bufferLength),
          20
        );
        ctx.fillStyle = "rgb(255, 0, 0)";
        ctx.fillRect(
          (canvas.width * rightChannelLevel) / bufferLength,
          0,
          canvas.width * (leftChannelLevel / bufferLength),
          20
        );
        requestAnimationFrame(draw);
      }
      draw();

      // 控制左声道的播放和暂停
      function playLeft() {
        if (!isPlayingLeft) {
          analyserNode.forEachChannel(function (channel) {
            channel.start(); //播放左声道音频流
          }, 1); // 从第一个（左）声道开始遍历
          isPlayingLeft = true;
        } else {
          analyserNode.forEachChannel(function (channel) {
            channel.stop(); //暂停左声道音频流
          }, 1); // 从第一个（左）声道开始遍历
          isPlayingLeft = false;
        }
      };

      function pauseLeft() {
        // 控制右声道的播放和暂停的方法类似，只需更改声道的索引即可，
        if (!isPlayingRight) {
          analyserNode.forEachChannel(function (channel) {
            channel.start(); //播放右声道音频流
          }, 2); // 从第二个（右）声道开始遍历
          isPlayingRight = true;
        } else {
          analyserNode.forEachChannel(function (channel) {
            channel.stop(); //暂停右声道音频流
          }, 2); // 从第二个（右）声道开始遍历
          isPlayingRight = false;
        }
      }
    </script>
  </body>
</html>
