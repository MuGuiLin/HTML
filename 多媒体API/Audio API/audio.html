<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Web Audio API examples: MediaElementAudioSource()</title>
  </head>

  <body>
    <h1>Web Audio API examples: MediaElementAudioSource()</h1>
    <p>
      Position the cursor vertically to change the volume (down is louder).
      将光标垂直放置以更改音量（向下音量更大）。
    </p>

    <audio mute controls>
      <source src="./media/hlbb.mp3" type="audio/mp3" />
      <source src="./media/viper.ogg" type="audio/ogg" />
      <p>This demo needs a browser supporting the &lt;audio&gt; element.</p>
    </audio>

    <br />
    <input type="range" id="range" min="0" max="1000" />
    <input type="range" min="0" max="100" id="volume" />
    <script>
      {
        /*
        let audioCtx;
        const audioElement = document.querySelector("audio");

        audioElement.addEventListener("play", () => {
        audioCtx = new AudioContext();
        // Create a MediaElementAudioSourceNode
        // Feed the HTMLMediaElement into it
          let source = new MediaElementAudioSourceNode(audioCtx, {
            mediaElement: audioElement,
          });

          // Create a gain node
          let gainNode = new GainNode(audioCtx);

          // Create variables to store mouse pointer Y coordinate
          // and HEIGHT of screen
          let currentY;
          let height = window.innerHeight;

          // Get new mouse pointer coordinates when mouse is moved
          // then set new gain value

          document.onmousemove = (e) => {
            currentY = window.Event
              ? e.pageY
              : event.clientY +
                (document.documentElement.scrollTop
                  ? document.documentElement.scrollTop
                  : document.body.scrollTop);

            gainNode.gain.value = currentY / height;
          };
          
          //将AudioBufferSourceNode连接到gainNode
          //和gainNode到目的地，这样我们就可以玩
          //音乐并使用鼠标光标调整音量
          source.connect(gainNode);
          gainNode.connect(audioCtx.destination);
        });
       */
      }

      {
        const audio = document.querySelector("audio");
        const ac = new (window.AudioContext || window.webkitAudioContext)();

       

        const source = ac.createMediaElementSource(audio);

        const splitter = new ChannelSplitterNode(ac, {
          numberOfOutputs: 2, // 默认6
        });
        source.connect(splitter);

         ac.decodeAudioData(someStereoBuffer, (data) => {
          const source = ac.createBufferSource();
          source.buffer = data;
          const splitter = ac.createChannelSplitter(2);
          source.connect(splitter);
          const merger = ac.createChannelMerger(2);

          // 仅减小左声道的音量
          const gainNode = ac.createGain();
          gainNode.gain.setValueAtTime(0.5, ac.currentTime);
          splitter.connect(gainNode, 0);

          //将拆分器连接回合并的第二个输入：我们有效地交换通道，在这里，反转立体图像。
          gainNode.connect(merger, 0, 1);
          splitter.connect(merger, 1, 0);

          const dest = ac.createMediaStreamDestination();

          // 因为我们使用了ChannelMergerNode，所以现在我们有了立体声
          //我们可以使用MediaStream将Web音频图管道传输到WebRTC，
          //MediaRecorder等。
          merger.connect(dest);
        });

        console.log(splitter);

        //   ac.decodeAudioData(someStereoBuffer, function (data) {
        //     var source = ac.createBufferSource();
        //     source.buffer = data;
        //     var splitter = ac.createChannelSplitter(2);
        //     source.connect(splitter);
        //     var merger = ac.createChannelMerger(2);

        //     // Reduce the volume of the left channel only
        //     var gainNode = ac.createGain();
        //     gainNode.gain.value = 0.5;
        //     splitter.connect(gainNode, 0);

        //     // Connect the splitter back to the second input of the merger: we
        //     // effectively swap the channels, here, reversing the stereo image.
        //     gainNode.connect(merger, 0, 1);
        //     splitter.connect(merger, 1, 0);

        //     var dest = ac.createMediaStreamDestination();

        //     // Because we have used a ChannelMergerNode, we now have a stereo
        //     // MediaStream we can use to pipe the Web Audio graph to WebRTC,
        //     // MediaRecorder, etc.
        //     merger.connect(dest);
        //   });
      }
    </script>

    <script type="text/javascript">
      //   var tank = new Audio("http://qianduannotes.duapp.com/file/tankWar.mp3");
      //   tank.loop = true;
      //   var mario = new Audio(
      //     "http://qianduannotes.duapp.com/file/SuperMario.mp3"
      //   );
      //   mario.loop = true;

      //   var AudioContext = AudioContext || webkitAudioContext;
      //   var context = new AudioContext();
      //   var source1 = context.createMediaElementSource(tank);
      //   var source2 = context.createMediaElementSource(mario);
      //   var gain1 = context.createGain();
      //   var gain2 = context.createGain(); //连接：source → gain → destination    source1.connect(gain1);
      //   source2.connect(gain2);
      //   gain1.connect(context.destination);
      //   gain2.connect(context.destination); //音量控制
      //   var value;
      //   onload = volume.onchange = function () {
      //     gain1.gain.value = volume.value / 100;
      //     gain2.gain.value = 1 - volume.value / 100;
      //   };

      //   tank.onload = mario.onlond = function () {
      //     console.log("var1, var2");
      //   };
      //   tank.play();
      //   mario.play();
    </script>
  </body>
</html>
