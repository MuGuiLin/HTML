<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Web createScriptProcessor and createAnalyser API 音频信息提取</title>
    <style>
      body {
        padding: 100px;
      }
      button, div {
        margin: 10px;
        font-size: 18px;
      }
      canvas {
        display: block;
        margin: 10px;
        height: 200px;
        border: 1px solid gray;
      }
      progress, canvas {
        width: 800px;
      }
    </style>
  </head>
  <body>
    <h1>Web createScriptProcessor and createAnalyser API 音频信息提取</h1>
    <hr />

    <audio loop src="./media/hlbb.mp3" controls ></audio>
    <button type="button" id="play">播放 / 暂停</button>
    <button type="button" id="volume">全部静音</button>

    <video loop src="./media/AAC-5.1.mp4" controls ></video>

    <br>
    左声道 <progress id="VUL" max="100" value="0"></progress><button type="button" id="VUL0">静音（L）</button>
    <br>
    右声道 <progress id="VUR" max="100" value="0"></progress><button type="button" id="VUR0">静音（R）</button>
    <br>
    <canvas id="canvas"></canvas>
    
    <div>
      <button type="button" id="fPlay">播放 fetch()加载的音频资源 </button>
      <button type="button" id="fStop">停止 fetch()加载的音频资源 </button>
      <button type="button" id="fL">静音左声道 </button>
      <button type="button" id="fR">静音右声道</button>
     </div>
    <script>
      let ac = null;
      try {
        ac = new (window.AudioContext || window.webkitAudioContext)();
      } catch (e) {
        alert("请更新至最新版的 Chrome 或者 Firefox");
      }

      // 音频源
      const audio = document.querySelector("audio");
      // const audio = document.querySelector("video");

      /*
      udioContext 获取 audio 源的原理是这样的：
        sourceNode -> scriptProcessor -> destinationNode
        1、audio有一个内置的输出通道
        2、AudioContext 通过 createMediaElementSource 将 audio 的输出直接拉去到新的环境中，之前 audio 环境被破坏了， 所以一定要连接  audioSource.connect(ac.destination)
        3、拉去的 audioSource 没有 start 函数，他会一直监听 audio 的操控，当 play 函数被触发的时候，开始播放音频。也可以认为，play 函数触发了 start （老版浏览器中是 noteOn）
      */
      const audioSource = ac.createMediaElementSource(audio);
      audioSource.connect(ac.destination); //注：要将音频源连接到AudioContext上下文中
      console.log(audioSource);

      play.onclick = () => {
        audio.paused ? audio.play() : audio.pause();
      };
      
      volume.onclick = () => {
        audio.volume = Number(!audio.volume);
      };

      // 提取方法1： createScriptProcessor() 
      function one () {
        const scriptProcessor = ac.createScriptProcessor(2048, 2, 2);
        audioSource.connect(scriptProcessor); // 注：要将scriptProcessor 连接到 audioSource 后e.inputBuffer.getChannelData(0)才能获取到音频信息
        console.log('scriptProcessor', scriptProcessor);

        const audioData = [[], []];
        // 性能不好
        scriptProcessor.onaudioprocess = function (e) {
          let left = e.inputBuffer.getChannelData(0);
          let right = e.inputBuffer.getChannelData(1);

          // 对左右声道进行处理，如：计算音量大小
          for (let i = 0; i < left.length; i++) {
            audioData[0][i] = left[i];
            audioData[1][i] = right[i];
          }
          console.log(audioData);
        };

        audio.addEventListener("playing", function () {
          scriptProcessor.connect(ac.destination); //注：要将scriptProcessor 连接到 AudioContext 后才会触发 onaudioprocess
        });

        audio.addEventListener("pause", function () {
          scriptProcessor.disconnect();
          ac.disconnect();
        });
        
        audio.addEventListener("ended", function () {
          scriptProcessor.disconnect();
          ac.disconnect();
        });
      };
      // one();




      // 提取方法2： createAnalyser()  // 可以用来获取音频时间和频率数据，以及实现数据可视化。
      function two () {
        const analyser = ac.createAnalyser();
        audioSource.connect(analyser); // 注：要将analyser 连接到 audioSource 后analyser.getByteTimeDomainData(audioData)才能获取到音频信息
        analyser.connect(ac.destination);  //注：要将scriptProcessor 连接到 AudioContext 后才会触发 onaudioprocess
        console.log('analyser', analyser);

        // 注：对“AnalyserNode”执行“getByteTimeDomainData”：参数1的类型只能是“Uint8Array”。
        // analyser.fftSize = 4096;
        // const audioData = new Uint16Array(analyser.fftSize);

        analyser.fftSize = 2048; // 默认2048;
        const audioData = new Uint8Array(analyser.fftSize);
        console.log(audioData);


        const canvas =  document.querySelector("#canvas");
        const ctx = canvas.getContext('2d');
        let timer = null;

        function animate() {
          // 用 requestAnimationFrame() 来反复获取时域数据;
          analyser.getByteTimeDomainData(audioData);

          ctx.fillStyle = 'rgb(200, 200, 200)';
          ctx.fillRect(0, 0, canvas.width, canvas.height);

          ctx.lineWidth = 2;
          ctx.strokeStyle = 'rgb(0, 0, 255)';

          ctx.beginPath();

          const sliceWidth = canvas.width * 1.0 / analyser.fftSize;
          let x = 0;
          for(let i = 0; i < analyser.fftSize; i++) {
            const v = audioData[i] / 128.0;
            const y = v * canvas.height / 2;
            if(i === 0) {
              ctx.moveTo(x, y);
            } else {
              ctx.lineTo(x, y);
            }
            x += sliceWidth;
          }

          ctx.lineTo(canvas.width, canvas.height / 2);
          ctx.stroke();
  
          timer = requestAnimationFrame(animate);
        };

        audio.addEventListener("playing", function () {
          animate();
        });

        audio.addEventListener("ended", function () {
          cancelAnimationFrame(timer);
        });

        audio.addEventListener("pause", function () {
          cancelAnimationFrame(timer);
        });
      }
      two();

      // createAnalyser() 双声道
      function two2(){
        const analyserL = ac.createAnalyser();
        const analyserR = ac.createAnalyser();

        audioSource.connect(analyserL);
        audioSource.connect(analyserR);
        audioSource.connect(ac.destination);

        const L = new StereoPannerNode(ac, {pan: -1});
        const R = new StereoPannerNode(ac, {pan: 1});

        analyserL.fftSize = 32;
        const bufferLengthL = analyserL.frequencyBinCount;
        const frequencyDataL = new Float32Array(bufferLengthL);

        analyserR.fftSize = 32;
        const bufferLengthR = analyserR.frequencyBinCount;
        const frequencyDataR = new Float32Array(bufferLengthR);

        let i = 0, job, origin = new Date().getTime();
        const timer = () => {
          if (new Date().getTime() - i > origin){
            analyserL.getFloatFrequencyData(frequencyDataL);
            analyserR.getFloatFrequencyData(frequencyDataR);

            let vL = Object.values(frequencyDataL).reduce((a,b) => a + b, 0)*-0.05
            let vR = Object.values(frequencyDataR).reduce((a,b) => a + b, 0)*-0.05

            VUL.value = ~~vL
            VUR.value = ~~vR

            i = i + 41 // 24 FPS
            job = requestAnimationFrame(timer);
          } else if (job !== null){
              requestAnimationFrame(timer)
          }
        }

        VUL0.onclick = () => {
          L.pan.value = Number(!L.pan.value);
          console.log('L', L, analyserL.gain)
        }
        VUR0.onclick = () => {
          R.pan.value = Number(!R.pan.value);
          console.log('R', R, R.pan.value)
        }
        requestAnimationFrame(timer)
      }

      two2()




      // 提取方法3：fetch() 加载音频资源
      {
        fetch("./media/hlbb.mp3")
          .then((response) => {
            return response.arrayBuffer();
          })
          .then((arrayBuffer) => {
            return ac.decodeAudioData(arrayBuffer);
          })
          .then((audioBuffer) => {
            // 获取音频源并创建两个独立的音频源
            let sourceLeft = ac.createBufferSource();
            let sourceRight = ac.createBufferSource();

            // 分别将音频数据赋值给左右两个音频源
            sourceLeft.buffer = audioBuffer;
            sourceRight.buffer = audioBuffer;

            // 创建两个分轨并分别连接至音频源
            let trackLeft = ac.createGain();
            let trackRight = ac.createGain();

            sourceLeft.connect(trackLeft);
            sourceRight.connect(trackRight);

            trackLeft.connect(ac.destination);
            trackRight.connect(ac.destination);

            console.log(sourceLeft, trackLeft)

            // 开始播放
            fPlay.onclick = () => {
              sourceLeft.start();
              // sourceRight.start();
            };

            // 停止播放
            fStop.onclick = () => {
              sourceLeft.stop();
              // sourceRight.start();
            };

            fL.onclick = () => {
              trackLeft.gain.value = Number(!trackLeft.gain.value);
            };

            fR.onclick = () => {
              trackRight.gain.value = Number(!trackRight.gain.value);
            };
          });
      }
    </script>
  </body>
</html>
